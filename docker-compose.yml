networks:
  mynetwork:
    driver: bridge

services:
  teleaiagent:
    build:
      context: .
      dockerfile: Dockerfile-teleaiagent
    container_name: teleaiagent
    env_file:
      - .env
    volumes:
      - ./volumes/teleaiagent/audio:/app/audio
      - ./volumes/teleaiagent/cache:/root/.cache  # Persist model cache (CPU-optimized)
      - ./volumes/teleaiagent/context:/app/context
      - ./volumes/teleaiagent/documents:/app/documents
      - ./volumes/teleaiagent/images:/app/images
      - ./volumes/teleaiagent/videos:/app/videos
      - ./volumes/teleaiagent/voice:/app/voice
      - ./volumes/teleaiagent/logs:/app/logs
    networks:
      - mynetwork
    restart: unless-stopped
    depends_on:
      - tagger
    # Resource limits for CPU-only operation
    deploy:
      resources:
        limits:
          memory: 2G  # Sufficient for SentenceTransformers on CPU
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  tagger:
    build:
      context: .
      dockerfile: Dockerfile-tagger
    container_name: tagger
    env_file:
      - .env
    ports:
      - "7777:7777"  # Expose tagger API port
    volumes:
      - ./volumes/images:/app/volume_images  # Shared volume for image storage
      - ./volumes/tagger/logs:/app/logs  # Separate logs for tagger
      - ./volumes/tagger/cache:/root/.cache  # Separate cache for tagger
    networks:
      - mynetwork
    restart: unless-stopped
    depends_on:
      - qdrant
      - ollama
    # Resource limits for CPU-only operation with AI processing
    deploy:
      resources:
        limits:
          memory: 3G  # More memory for image processing + AI models
        reservations:
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7777/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  ollama:
    image: ollama/ollama:0.11.11
    container_name: ollama-server
    volumes:
      - ./volumes/ollama:/root/.ollama
    networks:
      - mynetwork
    restart: unless-stopped
    # CPU-only optimizations for ARM64/x86_64 (no GPU)
    environment:
      - OLLAMA_NUM_PARALLEL=1         # CPU-only: conservative parallelism
      - OLLAMA_MAX_LOADED_MODELS=1    # CPU-only: one model at a time
      - OLLAMA_MAX_QUEUE=2            # CPU-only: smaller queue
      - OLLAMA_DEBUG=false            # Performance: disable debug
      - OLLAMA_HOST=0.0.0.0:11434     # Explicit binding
      - OLLAMA_ORIGINS=*              # Allow all origins
      - OLLAMA_KEEP_ALIVE=30m         # Keep models loaded longer
      - OLLAMA_NOPRUNE=true           # CPU-only: keep models cached
      - OLLAMA_MAX_VRAM=0             # CPU-only: disable GPU memory
      - OLLAMA_LLM_LIBRARY=cpu        # Force CPU backend
    deploy:
      resources:
        limits:
          memory: 12G                 # 16GB system: generous memory limit
          cpus: '4.0'                 # Use all available cores
        reservations:
          memory: 6G                  # 16GB system: adequate reservation
          cpus: '2.0'                 # Minimum 2 cores
    privileged: false
    security_opt:
      - no-new-privileges:true

  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API (optional)
    volumes:
      - ./volumes/qdrant/storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    env_file:
      - .env
    networks:
      - mynetwork
