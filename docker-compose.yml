networks:
  mynetwork:
    driver: bridge

services:
  teleaiagent:
    build:
      context: .
      dockerfile: Dockerfile-teleaiagent
    container_name: teleaiagent
    env_file:
      - .env
    volumes:
      - ./volumes/teleaiagent/audio:/app/audio
      - ./volumes/teleaiagent/cache:/root/.cache  # Persist model cache (CPU-optimized)
      - ./volumes/teleaiagent/context:/app/context
      - ./volumes/teleaiagent/documents:/app/documents
      - ./volumes/teleaiagent/images:/app/images
      - ./volumes/teleaiagent/videos:/app/videos
      - ./volumes/teleaiagent/voice:/app/voice
      - ./volumes/teleaiagent/logs:/app/logs
    networks:
      - mynetwork
    restart: unless-stopped
    depends_on:
      - tagger
    # Resource limits for CPU-only operation
    deploy:
      resources:
        limits:
          memory: 2G  # Sufficient for SentenceTransformers on CPU
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  tagger:
    build:
      context: .
      dockerfile: Dockerfile-tagger
    container_name: tagger
    env_file:
      - .env
    ports:
      - "7777:7777"  # Expose tagger API port
    volumes:
      - ./volumes/images:/app/volume_images  # Shared volume for image storage
      - ./volumes/tagger/logs:/app/logs  # Separate logs for tagger
      - ./volumes/tagger/cache:/root/.cache  # Separate cache for tagger
    networks:
      - mynetwork
    restart: unless-stopped
    depends_on:
      - qdrant
      - ollama
    # Resource limits for CPU-only operation with AI processing
    deploy:
      resources:
        limits:
          memory: 3G  # More memory for image processing + AI models
        reservations:
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7777/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  ollama:
    image: ollama/ollama:0.11.11
    container_name: ollama-server
    volumes:
      - ./volumes/ollama:/root/.ollama
    networks:
      - mynetwork
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:v1.11.0
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API (optional)
    volumes:
      - ./volumes/qdrant/storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    env_file:
      - .env
    networks:
      - mynetwork
